{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT/SJi3n4LVcdyRxTKaHq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wongdongwook/JSAC_MA-DeepSC/blob/main/MASCN_for_4Digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMV3c2ULoj6K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import pdb  # Debugging tool\n",
        "from torch import autograd  # For gradient penalty (WGAN-GP)\n",
        "import torchvision.utils as vutils  # For saving images\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm  # Progress bar\n",
        "import datetime\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix  # Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access .pt files or saved models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9JrgsjYrgKV",
        "outputId": "426eab77-104e-44c8-b83f-8245811e465d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set computing device (GPU if available, otherwise CPU)\n",
        "# Note: This code does not support Google TPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using ' + str(device).upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctDCiHJIrhKd",
        "outputId": "3188c79f-c802-4fec-ed2e-a04eb728664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "oLv24ebcsL2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset paths for different digit domains\n",
        "MNIST_path = '/content/drive/My Drive/StarGAN/dataset/MNIST_train.pt'\n",
        "MNISTM_path = '/content/drive/My Drive/StarGAN/dataset/MNISTM_train.pt'\n",
        "SynDigits_path = '/content/drive/My Drive/StarGAN/dataset/SYN_train.pt'\n",
        "USPS_path = '/content/drive/My Drive/StarGAN/dataset/USPS_train.pt'\n",
        "# Each .pt file is assumed to contain [image_tensor, label_tensor]\n",
        "\n",
        "class ImgDomainAdaptationData(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for multi-domain digit datasets.\n",
        "    Assigns domain indices as follows: MNIST=0, MNISTM=1, SYN=2, USPS=3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, w, h):\n",
        "        # Define image preprocessing (resize and normalize)\n",
        "        self.transform = transforms.Compose([transforms.Resize([w, h]),\n",
        "                                             transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "        # Load data from .pt files\n",
        "        self.MNIST_data = torch.load(MNIST_path)\n",
        "        self.MNISTM_data = torch.load(MNISTM_path)\n",
        "        self.SynDigits_data = torch.load(SynDigits_path)\n",
        "        self.USPS_data = torch.load(USPS_path)\n",
        "\n",
        "        # Apply preprocessing to image tensors\n",
        "        self.MNIST_img = self.transform(self.MNIST_data[0])\n",
        "        self.MNISTM_img = self.transform(self.MNISTM_data[0])\n",
        "        self.SynDigits_img = self.transform(self.SynDigits_data[0])\n",
        "        self.USPS_img = self.transform(self.USPS_data[0])\n",
        "\n",
        "        # Extract labels\n",
        "        self.MNIST_label = self.MNIST_data[1]\n",
        "        self.MNISTM_label = self.MNISTM_data[1]\n",
        "        self.SynDigits_label = self.SynDigits_data[1]\n",
        "        self.USPS_label = self.USPS_data[1]\n",
        "\n",
        "        # Apply additional preprocessing (e.g., channel repeat, domain indexing)\n",
        "        self.MNIST_img, self.MNIST_domain = self.pre_processing(self.MNIST_img, 0)\n",
        "        self.MNISTM_img, self.MNISTM_domain = self.pre_processing(self.MNISTM_img, 1)\n",
        "        self.SynDigits_img, self.SynDigits_domain = self.pre_processing(self.SynDigits_img, 2)\n",
        "        self.USPS_img, self.USPS_domain = self.pre_processing(self.USPS_img, 3)\n",
        "\n",
        "        # Concatenate all images, labels, and domain labels\n",
        "        self.img = torch.vstack((self.MNIST_img,\n",
        "                                  self.MNISTM_img,\n",
        "                                  self.SynDigits_img,\n",
        "                                  self.USPS_img))\n",
        "\n",
        "        self.label = torch.hstack((self.MNIST_label,\n",
        "                                    self.MNISTM_label,\n",
        "                                    self.SynDigits_label,\n",
        "                                    self.USPS_label))\n",
        "\n",
        "        self.domain = np.hstack((self.MNIST_domain,\n",
        "                                 self.MNISTM_domain,\n",
        "                                 self.SynDigits_domain,\n",
        "                                 self.USPS_domain))\n",
        "\n",
        "    def pre_processing(self, img, domain):\n",
        "        num_img = img.shape[0]\n",
        "\n",
        "        # Convert single-channel to 3-channel if needed\n",
        "        if len(img.shape) < 4:\n",
        "            img = img.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Generate domain label array\n",
        "        domain_label = np.zeros(num_img, dtype=int) + domain\n",
        "\n",
        "        return img, domain_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.img[index], self.label[index].item(), self.domain[index]\n"
      ],
      "metadata": {
        "id": "pxnWM6eysJGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "ZESpBghGx-0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_imgs(img, n_dms, gen, samples_path, channel_type, step=0, is_cuda=True):\n",
        "\t\"\"\"\n",
        "    Generate and save a grid of images showing original and domain-adapted results.\n",
        "\n",
        "    Args:\n",
        "        img: Input image batch (Tensor)\n",
        "        n_dms: Number of domains\n",
        "        gen: Generator model\n",
        "        samples_path: Directory to save image grids\n",
        "        channel_type: Channel type (e.g., 'AWGN')\n",
        "        step: Iteration step for file naming\n",
        "        is_cuda: Whether to use CUDA\n",
        "  \"\"\"\n",
        "\tgen.eval()\n",
        "\tm = img.shape[0]\n",
        "\n",
        "  # Create target domain labels: -1 for real, [0, ..., n_dms-1] for fake\n",
        "\tlbl = torch.arange(start=-1, end=n_dms)\n",
        "\tlbl = lbl.expand(m, n_dms+1).reshape([-1])\n",
        "\n",
        "\tif is_cuda:\n",
        "\t\tlbl = lbl.cuda()\n",
        "\t# Repeat input images for each domain label\n",
        "\timg_ = torch.repeat_interleave(img, n_dms+1, dim=0)\n",
        "\tSNR_TRAIN = torch.randint(20, 28, (img_.shape[0], 1)).cuda() # Random SNR values (simulating channel noise)\n",
        "\treal_idx = torch.arange(start=0, end=m*(n_dms+1), step=n_dms+1) # Mark real images (used as ground truth)\n",
        "\tlbl[real_idx] = 0\n",
        "\n",
        "\t# Generate images via semantic encoder+channel+decoder\n",
        "\tdisplay_imgs = gen(img_, channel_type,SNR_TRAIN, lbl)\n",
        "\tdisplay_imgs[real_idx] = img\n",
        "\n",
        "\t# Create grid and save to file\n",
        "\tdisplay_imgs_ = vutils.make_grid(display_imgs, normalize=True, nrow=n_dms+1, padding=2, pad_value=1)\n",
        "\tvutils.save_image(display_imgs_, os.path.join(samples_path, 'sample_' + str(step) + '.png'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# WGAN-GP 구조에서 쓰이는 gradient penalty 계산\n",
        "# interpolated image를 만든 후, critic (discriminator)의 출력을 기반으로 그래디언트의 L2 norm을 계산\n",
        "def gradient_penalty(real, fake, critic, is_cuda=True):\n",
        "\t\"\"\"\n",
        "    Compute the gradient penalty term for WGAN-GP.\n",
        "\n",
        "    Args:\n",
        "        real: Real images (Tensor)\n",
        "        fake: Generated images (Tensor)\n",
        "        critic: Discriminator (a.k.a. critic)\n",
        "        is_cuda: Whether CUDA is used\n",
        "\n",
        "    Returns:\n",
        "        Gradient penalty (scalar Tensor)\n",
        "\t\"\"\"\n",
        "\tm = real.shape[0]\n",
        "\tepsilon = torch.rand(m, 1, 1, 1)  # Sample random weights for interpolation\n",
        "\tif is_cuda:\n",
        "\t\tepsilon = epsilon.cuda()\n",
        "\n",
        "\t# Interpolated images between real and fake\n",
        "\tinterpolated_img = epsilon * real + (1-epsilon) * fake\n",
        "\tinterpolated_out, _,_= critic(interpolated_img)\n",
        "\n",
        "\t# Compute gradients of critic outputs w.r.t. interpolated inputs\n",
        "\tgrads = autograd.grad(outputs=interpolated_out, inputs=interpolated_img,\n",
        "\t\t\t\t\t\t\t   grad_outputs=torch.ones(interpolated_out.shape).cuda() if is_cuda else torch.ones(interpolated_out.shape),\n",
        "\t\t\t\t\t\t\t   create_graph=True, retain_graph=True)[0]\n",
        "\tgrads = grads.reshape([m, -1])\n",
        "\tgrad_penalty = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\treturn grad_penalty\n"
      ],
      "metadata": {
        "id": "m55-_BnVv9m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "9Z3h63UtyWWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
        "    module = []\n",
        "    if transpose:\n",
        "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "    else:\n",
        "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "    if use_bn:\n",
        "        module.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*module)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        self.conv2 = conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return x + self.conv2(x)\n",
        "\n",
        "# Original Existing Works\n",
        "class AF_block(nn.Module):\n",
        "    def __init__(self, Nin, Nh, No):\n",
        "        super(AF_block, self).__init__()\n",
        "        self.fc1 = nn.Linear(Nin+1, Nh)\n",
        "        self.fc2 = nn.Linear(Nh, No)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, snr):\n",
        "        # out = F.adaptive_avg_pool2d(x, (1,1))\n",
        "        # out = torch.squeeze(out)\n",
        "        # out = torch.cat((out, snr), 1)\n",
        "        if snr.shape[0]>1:\n",
        "            snr = snr.squeeze()\n",
        "        snr = snr.unsqueeze(1)\n",
        "        mu = torch.mean(x, (2, 3))\n",
        "        out = torch.cat((mu, snr), 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.unsqueeze(2)\n",
        "        out = out.unsqueeze(3)\n",
        "        out = out*x\n",
        "        return out\n",
        "\n",
        "\n",
        "## Power normalization before transmission\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1)\n",
        "    return np.sqrt(P*z_dim)*z/z_M.t()\n",
        "\n",
        "def Power_norm_complex(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_com = torch.complex(z[:, 0:z_dim:2], z[:, 1:z_dim:2])\n",
        "    z_com_conj = torch.complex(z[:, 0:z_dim:2], -z[:, 1:z_dim:2])\n",
        "    z_power = torch.sum(z_com*z_com_conj, 1).real\n",
        "    z_M = z_power.repeat(z_dim//2, 1)\n",
        "    z_nlz = np.sqrt(P*z_dim)*z_com/torch.sqrt(z_M.t())\n",
        "    z_out = torch.zeros(batch_size, z_dim).cuda()\n",
        "    z_out[:, 0:z_dim:2] = z_nlz.real\n",
        "    z_out[:, 1:z_dim:2] = z_nlz.imag\n",
        "    return z_out\n",
        "\n",
        "# The (real) AWGN channel\n",
        "def AWGN_channel(x, snr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def AWGN_complex(x, snr, Ps = 1):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    n_I = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    n_R = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "    y = x + noise\n",
        "    return y\n",
        "\n",
        "# Please set the symbol power if it is not a default value\n",
        "def Fading_channel(x, snr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm_VLC(z, cr, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    Kv = torch.ceil(z_dim*cr).int()\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1).cuda()\n",
        "    return torch.sqrt(Kv*P)*z/z_M.t()\n",
        "\n",
        "def AWGN_channel_VLC(x, snr, cr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    mask = mask_gen(length, cr).cuda()\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(1, length).cuda()\n",
        "    noise = noise*mask\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def Fading_channel_VLC(x, snr, cr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    mask = mask_gen(K, cr).cuda()\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)*mask\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "def Channel(z, snr, channel_type = 'AWGN'):\n",
        "    z = Power_norm(z)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel(z, snr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel(z, snr)\n",
        "    return z\n",
        "\n",
        "def Channel_VLC(z, snr, cr, channel_type = 'AWGN'):\n",
        "    z = Power_norm_VLC(z, cr)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel_VLC(z, snr, cr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel_VLC(z, snr, cr)\n",
        "    return z\n",
        "\n",
        "def mask_gen(N, cr, ch_max = 48): # channel 갯수에 따라 달라짐. 48,8,8\n",
        "    MASK = torch.zeros(cr.shape[0], N).int()\n",
        "    nc = N//ch_max\n",
        "    #print(nc) # nc :64\n",
        "    for i in range(0, cr.shape[0]):\n",
        "        L_i = nc*torch.round(ch_max*cr[i]).int()\n",
        "        MASK[i, 0:L_i] = 1\n",
        "    return MASK\n",
        "\n",
        "class ADJSCC(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = Channel(z, snr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out\n",
        "\n",
        "# The DeepJSCC_V model, also called ADJSCC_V\n",
        "class ADJSCC_V(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC_V, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, cr, channel_type = 'AWGN'):\n",
        "        # x: [128, 3, 32, 32], snr, cr : [128, 1]\n",
        "        z = self.encoder(x, snr) # encoder output depend on cr. if cr= 1, output : [128,3 *32*32 /cr]\n",
        "        z = z*mask_gen(z.shape[1], cr).cuda()\n",
        "        z = Channel_VLC(z, snr, cr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out # [3, 32, 32]\n",
        "\n"
      ],
      "metadata": {
        "id": "Gufuhl88yYSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Model (Common)\n",
        "MDAN과 MASCN은 판별기 모델은 같은걸 쓴다고 가정함"
      ],
      "metadata": {
        "id": "oPb6DIcmyt4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels=3, num_domains=5, num_classes=10, image_size=32, conv_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = conv_block(channels, conv_dim, use_bn=False)\n",
        "        self.conv2 = conv_block(conv_dim, conv_dim * 2, use_bn=False)\n",
        "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4, use_bn=False)\n",
        "        self.conv4 = conv_block(conv_dim * 4, conv_dim * 8, use_bn=False)\n",
        "\n",
        "        self.gan = conv_block(conv_dim * 8, 1, k_size=3, stride=1, pad=1, use_bn=False)\n",
        "        self.cls = conv_block(conv_dim * 8, num_domains, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "        self.label_cls = conv_block(conv_dim * 8, num_classes, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        alpha = 0.01\n",
        "        x = F.leaky_relu(self.conv1(x), alpha)\n",
        "        x = F.leaky_relu(self.conv2(x), alpha)\n",
        "        x = F.leaky_relu(self.conv3(x), alpha)\n",
        "        x = F.leaky_relu(self.conv4(x), alpha)\n",
        "        gan_out = self.gan(x)\n",
        "        cls_out = self.cls(x)\n",
        "        label_cls_out = self.label_cls(x)\n",
        "\n",
        "        return gan_out, cls_out.squeeze(), label_cls_out.squeeze()"
      ],
      "metadata": {
        "id": "42QCSTB6ysLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDAN (Multi domain data adaptaion network)\n",
        "Original StarGAN 코드"
      ],
      "metadata": {
        "id": "3gpw8eWPywUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):  # Original StarGAN. 인코더 앞에 위치함.\n",
        "    def __init__(self, in_channels=3, num_domains=5, image_size=32, out_channels=3, conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.embed_layer = nn.Embedding(num_domains, image_size**2)\n",
        "\n",
        "        self.conv1 = conv_block(in_channels+1, conv_dim, k_size=5, stride=1, pad=2, use_bn=True) # 64\n",
        "        self.conv2 = conv_block(conv_dim, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True) #  64 * 2\n",
        "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4, k_size=4, stride=2, pad=1, use_bn=True) # 64 *4 = 256\n",
        "        self.res4 = ResBlock(conv_dim * 4)\n",
        "        self.res5 = ResBlock(conv_dim * 4)\n",
        "        self.res6 = ResBlock(conv_dim * 4)\n",
        "        self.tconv7 = conv_block(conv_dim * 4, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.tconv8 = conv_block(conv_dim * 2, conv_dim, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.conv9 = conv_block(conv_dim, out_channels, k_size=5, stride=1, pad=2, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, target_dm=None):\n",
        "        if target_dm is None:\n",
        "            target_dm = torch.ones(x.shape[0])\n",
        "        target_dm = target_dm.long()\n",
        "        embed = self.embed_layer(target_dm).reshape([-1, 1, self.image_size, self.image_size])\n",
        "        x = torch.cat((x, embed), dim=1) #[3, 32, 32] + [1, 32, 32] = [4, 32, 32]\n",
        "        x = F.relu(self.conv1(x)) # [64,32,32]\n",
        "        x = F.relu(self.conv2(x))  # [128,16,16]\n",
        "        x = F.relu(self.conv3(x))# [256, 8, 8]\n",
        "        x = F.relu(self.res4(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.res5(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.res6(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.tconv7(x)) # [128,16,16]\n",
        "        x = F.relu(self.tconv8(x))  # [64,32,32]\n",
        "        x = torch.tanh(self.conv9(x))  # [3,32,32]\n",
        "        return x"
      ],
      "metadata": {
        "id": "zewqdDMy1aRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MASCN (Multi domain adaptive semantic coding network)"
      ],
      "metadata": {
        "id": "F5pJTRywsUNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):  # StarGAN based Generative Semantic Transceiver\n",
        "    def __init__(self, in_channels=3, num_domains=5, image_size=32, out_channels=3, conv_dim=64, CR=0.2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.embed_layer = nn.Embedding(num_domains, image_size**2)\n",
        "        self.CR=CR\n",
        "        conv_dim_btl = int(conv_dim * 4 *CR)\n",
        "        self.conv_dim=conv_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = conv_block(in_channels+1, conv_dim, k_size=5, stride=1, pad=2, use_bn=True) # 64\n",
        "        self.conv2 = conv_block(conv_dim, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True) #  64*2\n",
        "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4, k_size=4, stride=2, pad=1, use_bn=True) # 64*4\n",
        "        self.res4 = ResBlock(conv_dim * 4)\n",
        "        self.AF4= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res5 = ResBlock(conv_dim * 4)\n",
        "        self.AF5= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.conv5 = conv_block(conv_dim * 4, conv_dim_btl , k_size=3, stride=1, pad=1, use_bn=True)\n",
        "\n",
        "        # Decoder\n",
        "        self.conv6 = conv_block(conv_dim_btl, conv_dim * 4, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        self.AF6= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res6 = ResBlock(conv_dim * 4)\n",
        "        self.AF7= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res7 = ResBlock(conv_dim * 4)\n",
        "        self.tconv7 = conv_block(conv_dim * 4, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.tconv8 = conv_block(conv_dim * 2, conv_dim, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.conv9 = conv_block(conv_dim, out_channels, k_size=5, stride=1, pad=2, use_bn=False)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, channel_type, snr,  target_dm=None):\n",
        "        # Encoder\n",
        "        if target_dm is None:\n",
        "            target_dm = torch.ones(x.shape[0])\n",
        "        target_dm = target_dm.long()\n",
        "        embed = self.embed_layer(target_dm).reshape([-1, 1, self.image_size, self.image_size])\n",
        "        x = torch.cat((x, embed), dim=1)\n",
        "        x = F.relu(self.conv1(x)) # [12,32,32]\n",
        "        x = F.relu(self.conv2(x))  # [24,16,16]\n",
        "        x = F.relu(self.conv3(x))# [48, 8, 8]\n",
        "        x = F.relu(self.res4(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF4(x, snr))\n",
        "        x = F.relu(self.res5(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF5(x, snr))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # AWGN channel\n",
        "        x = Channel(x, snr, channel_type)\n",
        "         # Decoder\n",
        "        out_size= int(self.conv_dim*4 *self.CR)\n",
        "        x = x.view(-1, out_size, 8, 8)\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.AF6(x, snr))\n",
        "        x = F.relu(self.res6(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF7(x, snr))\n",
        "        x = F.relu(self.res7(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.tconv7(x)) # [24,16,16]\n",
        "        x = F.relu(self.tconv8(x))  # [12,32,32]\n",
        "        x = torch.tanh(self.conv9(x))  # [3,32,32]\n",
        "        return x"
      ],
      "metadata": {
        "id": "GHoxCtjX1W3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "OdmId_VIswkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#하이퍼파라미터 설정\n",
        "EPOCHS = 100  # 전체 학습 epoch 수 (보통 50~300에서 튜닝)\n",
        "BATCH_SIZE = 128 # 한 배치당 이미지 수\n",
        "IMGS_TO_DISPLAY = 25 # 시각화용 이미지 개수\n",
        "\n",
        "IMAGE_SIZE = 32  # 입력 이미지 크기 (32x32)\n",
        "NUM_DOMAINS = 4   # 도메인 수: MNIST, MNIST-M, SYN, USPS\n",
        "\n",
        "N_CRITIC = 5  # Discriminator를 Generator보다 몇 배 더 학습할지 (WGAN-GP 기준)\n",
        "GRADIENT_PENALTY = 10  # WGAN-GP의 lambda 값\n",
        "CR=1   # Compression Ratio 관련 하이퍼파라미터\n",
        "\n",
        "# 경로 설정 및 디렉토리 생성\n",
        "model_path = '/content/drive/My Drive/StarGAN/model_256_AWGN_CR_0.1_AFB'\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "samples_path = '/content/drive/My Drive/StarGAN/samples_256_AWGN_CR_0.1_AFB'\n",
        "os.makedirs(samples_path, exist_ok=True)\n",
        "\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# Generator, Discriminator 모델 생성 및 조건부 불러오기\n",
        "gen = Generator(num_domains=NUM_DOMAINS, image_size=IMAGE_SIZE, CR=0.1) # Compression Ratio = 0.1인 모델 생성 및 초기\n",
        "dis = Discriminator(num_domains=NUM_DOMAINS, image_size=IMAGE_SIZE)\n",
        "\n",
        "if LOAD_MODEL: # 학습된 모델 저장 경로 (generator.pkl, discriminator.pkl)\n",
        "  gen.load_state_dict(torch.load(os.path.join(model_path, 'generator.pkl')))\n",
        "  dis.load_state_dict(torch.load(os.path.join(model_path, 'discriminator.pkl')))\n",
        "\n",
        "# gpu 디바이스 할당\n",
        "gen.to(device)\n",
        "dis.to(device)\n",
        "\n",
        "# Define Optimizers\n",
        "g_opt = torch.optim.Adam(gen.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "d_opt = torch.optim.Adam(dis.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "# Define Loss\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "# Data loaders\n",
        "data1 = ImgDomainAdaptationData(32,32)\n",
        "ds_loader = torch.utils.data.DataLoader(data1,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True,\n",
        "                                        num_workers=4,\n",
        "                                        drop_last=True)\n",
        "iters_per_epoch = len(ds_loader)\n",
        "\n",
        "# Fix images for viz\n",
        "loader_iter = iter(ds_loader)\n",
        "img_fixed = next(loader_iter)[0][:IMGS_TO_DISPLAY]\n",
        "\n",
        "# GPU Compatibility\n",
        "is_cuda = torch.cuda.is_available()\n",
        "gen, dis = gen.to(device), dis.to(device)\n",
        "img_fixed = img_fixed.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qMs5rhz1p9O",
        "outputId": "d7e2300b-e22a-498a-f804-5579be03776b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop for MASCN\n",
        "e.g., compression rate =0.1"
      ],
      "metadata": {
        "id": "lNo9Q7Pi-gQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_iter = 0\n",
        "g_label_loss= g_gan_loss = g_clf_loss = g_rec_loss = torch.Tensor([0])\n",
        "EPOCHS= 100\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  gen.train()\n",
        "  dis.train()\n",
        "\n",
        "  num_batches = 0\n",
        "\n",
        "  for i, data in enumerate(ds_loader):\n",
        "    total_iter += 1\n",
        "    num_batches += 1\n",
        "\n",
        "    # Loading data\n",
        "    real, label, dm = data\n",
        "    snr = torch.randint(0, 28, (real.shape[0], 1)).cuda()\n",
        "    real, label, dm = real.to(device), label.to(device), dm.long().to(device)\n",
        "\n",
        "    target_dm = dm[torch.randperm(dm.size(0))]\n",
        "\n",
        "    # Fake Images\n",
        "    fake = gen(real, 'AWGN', snr, target_dm)\n",
        "\n",
        "    # Training discriminator\n",
        "    real_gan_out, real_cls_out, real_label_out   = dis(real)\n",
        "    fake_gan_out, fake_cls_out, _  = dis(fake.detach())\n",
        "\n",
        "    d_gan_loss = -(real_gan_out.mean() - fake_gan_out.mean()) + gradient_penalty(real, fake, dis, is_cuda) * GRADIENT_PENALTY\n",
        "    d_clf_loss = ce(real_cls_out, dm)\n",
        "    d_label_loss = ce(real_label_out, label)  # Label classification loss for discriminator\n",
        "\n",
        "    d_opt.zero_grad()\n",
        "    d_loss = d_gan_loss + d_clf_loss + d_label_loss\n",
        "    d_loss.backward()\n",
        "    d_opt.step()\n",
        "\n",
        "    # Training Generator\n",
        "    if total_iter % N_CRITIC == 0:\n",
        "      fake = gen(real, 'AWGN', snr, target_dm)\n",
        "      fake_gan_out, fake_cls_out, fake_label_out  = dis(fake)\n",
        "\n",
        "      g_gan_loss = - fake_gan_out.mean()\n",
        "      g_clf_loss = ce(fake_cls_out, target_dm)\n",
        "      g_label_loss = ce(fake_label_out, label)  # Label classification loss for generator\n",
        "      g_rec_loss = (real - gen(fake, 'AWGN', snr, dm)).abs().mean()\n",
        "\n",
        "      g_opt.zero_grad()\n",
        "      g_loss = g_gan_loss + g_clf_loss + g_rec_loss +g_label_loss\n",
        "      g_loss.backward()\n",
        "      g_opt.step()\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"\\nEpoch: \" + str(epoch + 1) + \"/\" + str(EPOCHS)\n",
        "          + \" iter: \" + str(i+1) + \"/\" + str(iters_per_epoch)\n",
        "          + \" total_iters: \" + str(total_iter)\n",
        "          + \"\\td_gan_loss:\" + str(round(d_gan_loss.item(), 4))\n",
        "          + \"\\td_clf_loss:\" + str(round(d_clf_loss.item(), 4))\n",
        "          + \"\\td_label_loss:\" + str(round(d_label_loss.item(), 4))\n",
        "          + \"\\tg_gan_loss:\" + str(round(g_gan_loss.item(), 4))\n",
        "          + \"\\tg_clf_loss:\" + str(round(g_clf_loss.item(), 4))\n",
        "          + \"\\tg_rec_loss:\" + str(round(g_rec_loss.item(), 4))\n",
        "          + \"\\tg_label_loss:\" + str(round(g_label_loss.item(), 4)))\n",
        "\n",
        "    if total_iter % 100==0:\n",
        "      generate_imgs(img_fixed, NUM_DOMAINS, gen, samples_path, 'AWGN', total_iter, is_cuda)\n",
        "\n",
        "  # torch.save(gen.state_dict(), os.path.join(model_path, 'gen.pkl'))\n",
        "  # torch.save(dis.state_dict(), os.path.join(model_path, 'dis.pkl'))\n",
        "\n",
        "torch.save(gen.state_dict(), os.path.join(model_path, 'gen_AFB_100_0.1.pkl'))\n",
        "torch.save(dis.state_dict(), os.path.join(model_path, 'dis_AFB_100_0.1.pkl'))"
      ],
      "metadata": {
        "id": "zMrUtehi-9hG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBIZQLPVYvXEJrOEiPVkFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wongdongwook/JSAC_MA-DeepSC/blob/main/Evalutation_for_4digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGOP4TOVX03_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Function\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# if you use google TPU, this source code doesn't work. Cuz TPU is not GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using ' + str(device).upper())"
      ],
      "metadata": {
        "id": "ypHOGgftboCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ebda46-4eab-450c-e790-3b90540d094b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using CUDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "l89PgUWEWqgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_imgs(img, n_dms, gen, samples_path, device, step=0): #image visulation & save function\n",
        "    gen.eval()\n",
        "    m = img.shape[0]\n",
        "\n",
        "    lbl = torch.arange(start=-1, end=n_dms)\n",
        "    lbl = lbl.expand(m, n_dms+1).reshape([-1])\n",
        "    lbl = lbl.to(device)\n",
        "\n",
        "    img_ = torch.repeat_interleave(img, n_dms+1, dim=0).to(device)\n",
        "\n",
        "    real_idx = torch.arange(start=0, end=m*(n_dms+1), step=n_dms+1)\n",
        "    lbl[real_idx] = 0\n",
        "\n",
        "    display_imgs = gen(img_, lbl)\n",
        "    display_imgs[real_idx] = img\n",
        "\n",
        "    display_imgs_ = vutils.make_grid(\n",
        "        display_imgs, normalize=True, nrow=n_dms+1, padding=2, pad_value=1)\n",
        "    vutils.save_image(display_imgs_, os.path.join(\n",
        "        samples_path, f'sample_epoch_{step}.png'))\n",
        "\n",
        "    np_image = display_imgs_.cpu().detach().numpy()\n",
        "    np_image = np.transpose(np_image, (1, 2, 0))\n",
        "    # Plot using matplotlib\n",
        "    plt.figure(figsize=(8, 8)) # You can adjust the figure size as needed\n",
        "    plt.imshow(np_image)\n",
        "    plt.axis('off')  # Optional: Remove axes for a cleaner look\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8PzJmBMLWsXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "309M9V49WuJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to image domain datasets\n",
        "MNIST_train_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/MNIST_train.pt'\n",
        "MNISTM_train_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/MNISTM_train.pt'\n",
        "SYN_train_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/SYN_train.pt'\n",
        "USPS_train_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/USPS_train.pt'\n",
        "\n",
        "MNIST_test_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/MNIST_test.pt'\n",
        "MNISTM_test_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/MNISTM_test.pt'\n",
        "SYN_test_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/SYN_test.pt'\n",
        "USPS_test_path = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/USPS_test.pt'\n",
        "\n",
        "\n",
        "train_path = [MNIST_train_path, MNISTM_train_path, SYN_train_path, USPS_train_path]\n",
        "test_path = [MNIST_test_path, MNISTM_test_path, SYN_test_path, USPS_test_path]\n",
        "\n",
        "\n",
        "class ImgData(torch.utils.data.Dataset): # Basic dataset class for a single domain\n",
        "    def __init__(self, path, w, h):\n",
        "\n",
        "        self.transform = transforms.Compose([transforms.Resize([w, h]),\n",
        "                                             transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "        self.data = torch.load(path)\n",
        "\n",
        "        self.img = self.transform(self.data[0])\n",
        "        self.img = self.pre_processing(self.img)\n",
        "\n",
        "        self.label = self.data[1]\n",
        "\n",
        "        self.len = self.label.shape[0]\n",
        "\n",
        "\n",
        "    def pre_processing(self, img):\n",
        "        if len(img.shape) < 4:\n",
        "            img = img.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return self.img[index], self.label[index]\n",
        "\n",
        "\n",
        "class AllDomainExceptOne(torch.utils.data.Dataset):  # Combine all domains except one\n",
        "\n",
        "    def __init__(self, path_arr, w, h):\n",
        "\n",
        "        self.transform = transforms.Compose([transforms.Resize([w, h]),\n",
        "                                             transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "        self.data = []\n",
        "        self.img = []\n",
        "        self.label = []\n",
        "        for path in path_arr:\n",
        "            data = torch.load(path)\n",
        "            self.data.append(data)\n",
        "            self.img.append(self.pre_processing(self.transform(data[0])))\n",
        "            self.label.append(data[1])\n",
        "\n",
        "        self.img = torch.vstack(self.img)\n",
        "        self.label = torch.hstack(self.label)\n",
        "\n",
        "        self.len = self.label.shape[0]\n",
        "\n",
        "    def pre_processing(self, img):\n",
        "        if len(img.shape) < 4:\n",
        "            img = img.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.img[index], self.label[index]\n",
        "\n",
        "\n",
        "class AllDomainData(torch.utils.data.Dataset): # Dataset for all domains with optional domain labels\n",
        "\n",
        "    def __init__(self, w, h, is_training=True, DA_task=False):\n",
        "        # is_training = True if load dataset for training, False if otherwise\n",
        "        # DA_task = True to also output domain label, False if otherwise\n",
        "\n",
        "        self.transform = transforms.Compose([transforms.Resize([w, h]),\n",
        "                                            transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "        self.DA_task = DA_task\n",
        "\n",
        "        if is_training:\n",
        "            self.MNIST_data = torch.load(MNIST_train_path)\n",
        "            self.MNISTM_data = torch.load(MNISTM_train_path)\n",
        "            self.SYN_data = torch.load(SYN_train_path)\n",
        "            self.USPS_data = torch.load(USPS_train_path)\n",
        "\n",
        "        else:\n",
        "            self.MNIST_data = torch.load(MNIST_test_path)\n",
        "            self.MNISTM_data = torch.load(MNISTM_test_path)\n",
        "            self.SYN_data = torch.load(SYN_test_path)\n",
        "            self.USPS_data = torch.load(USPS_test_path)\n",
        "\n",
        "        self.MNIST_img = self.transform(self.MNIST_data[0])\n",
        "        self.MNISTM_img = self.transform(self.MNISTM_data[0])\n",
        "        self.SYN_img = self.transform(self.SYN_data[0])\n",
        "        self.USPS_img = self.transform(self.USPS_data[0])\n",
        "\n",
        "        self.MNIST_label = self.MNIST_data[1]\n",
        "        self.MNISTM_label = self.MNISTM_data[1]\n",
        "        self.SYN_label = self.SYN_data[1]\n",
        "        self.USPS_label = self.USPS_data[1]\n",
        "\n",
        "        if DA_task:\n",
        "            self.MNIST_img, self.MNIST_domain = self.pre_processing(self.MNIST_img, 0)\n",
        "            self.MNISTM_img, self.MNISTM_domain = self.pre_processing(self.MNISTM_img, 1)\n",
        "            self.SYN_img, self.SYN_domain = self.pre_processing(self.SYN_img, 2)\n",
        "            self.USPS_img, self.USPS_domain = self.pre_processing(self.USPS_img, 3)\n",
        "        else:\n",
        "            self.MNIST_img = self.pre_processing(self.MNIST_img)\n",
        "            self.MNISTM_img = self.pre_processing(self.MNISTM_img)\n",
        "            self.SYN_img = self.pre_processing(self.SYN_img)\n",
        "            self.USPS_img = self.pre_processing(self.USPS_img)\n",
        "\n",
        "        self.img = torch.vstack((self.MNIST_img,\n",
        "                                 self.MNISTM_img,\n",
        "                                 self.SYN_img,\n",
        "                                 self.USPS_img))\n",
        "\n",
        "        self.label = torch.hstack((self.MNIST_label,\n",
        "                                   self.MNISTM_label,\n",
        "                                   self.SYN_label,\n",
        "                                   self.USPS_label))\n",
        "\n",
        "        if DA_task:\n",
        "            self.domain = np.hstack((self.MNIST_domain,\n",
        "                                     self.MNISTM_domain,\n",
        "                                     self.SYN_domain,\n",
        "                                     self.USPS_domain))\n",
        "\n",
        "        self.len = self.label.shape[0]\n",
        "\n",
        "\n",
        "    def pre_processing(self, img, domain=None):\n",
        "\n",
        "        if len(img.shape) < 4:\n",
        "            img = img.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "        if domain is not None:\n",
        "            num_img = img.shape[0]\n",
        "            domain_label = np.zeros(num_img, dtype=int) + domain\n",
        "\n",
        "            return img, domain_label\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.DA_task:\n",
        "            return self.img[index], self.label[index], self.domain[index]\n",
        "\n",
        "        return self.img[index], self.label[index]"
      ],
      "metadata": {
        "id": "yNFIx3KAWvRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dada Adaptation  Model"
      ],
      "metadata": {
        "id": "HLnhPq8hW03F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN based DA Module"
      ],
      "metadata": {
        "id": "A8g7NlgiW4XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CycleGAN_conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
        "    module = []\n",
        "    if transpose:\n",
        "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, output_padding=pad, bias=not use_bn))\n",
        "    else:\n",
        "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "    if use_bn:\n",
        "        module.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*module)\n",
        "\n",
        "\n",
        "class CycleGAN_ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CycleGAN_ResBlock, self).__init__()\n",
        "        self.conv1 = CycleGAN_conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        self.conv2 = CycleGAN_conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return x + self.conv2(x)\n",
        "\n",
        "class CycleGAN_Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, conv_dim=64):\n",
        "        super(CycleGAN_Generator, self).__init__()\n",
        "        self.conv1 = CycleGAN_conv_block(in_channels, conv_dim, k_size=5, stride=1, pad=2, use_bn=True)\n",
        "        self.conv2 = CycleGAN_conv_block(conv_dim, conv_dim * 2, k_size=3, stride=2, pad=1, use_bn=True)\n",
        "        self.conv3 = CycleGAN_conv_block(conv_dim * 2, conv_dim * 4, k_size=3, stride=2, pad=1, use_bn=True)\n",
        "        self.res4 = CycleGAN_ResBlock(conv_dim * 4)\n",
        "        self.tconv5 = CycleGAN_conv_block(conv_dim * 4, conv_dim * 2, k_size=3, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.tconv6 = CycleGAN_conv_block(conv_dim * 2, conv_dim, k_size=3, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.conv7 = CycleGAN_conv_block(conv_dim, out_channels, k_size=5, stride=1, pad=2, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.res4(x))\n",
        "        x = F.relu(self.tconv5(x))\n",
        "        x = F.relu(self.tconv6(x))\n",
        "        x = torch.tanh(self.conv7(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "tvefZNvgW3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StarGAN based DA Module"
      ],
      "metadata": {
        "id": "E3GlHX65W6_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def starGAN_conv_block(c_in, c_out, k_size=4, stride=2, pad=1, use_bn=True, transpose=False):\n",
        "    module = []\n",
        "    if transpose:\n",
        "        module.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "    else:\n",
        "        module.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=not use_bn))\n",
        "    if use_bn:\n",
        "        module.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*module)\n",
        "\n",
        "class starGAN_ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(starGAN_ResBlock, self).__init__()\n",
        "        self.conv1 = starGAN_conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        self.conv2 = starGAN_conv_block(channels, channels, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return x + self.conv2(x)\n",
        "\n",
        "# Modified starGAN_Discriminator\n",
        "class starGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, channels=3, num_domains=5, num_classes=10, image_size=32, conv_dim=64):\n",
        "        super(starGAN_Discriminator, self).__init__()\n",
        "        self.conv1 = starGAN_conv_block(channels, conv_dim, use_bn=False)\n",
        "        self.conv2 = starGAN_conv_block(conv_dim, conv_dim * 2, use_bn=False)\n",
        "        self.conv3 = starGAN_conv_block(conv_dim * 2, conv_dim * 4, use_bn=False)\n",
        "        self.conv4 = starGAN_conv_block(conv_dim * 4, conv_dim * 8, use_bn=False)\n",
        "\n",
        "        self.gan = starGAN_conv_block(conv_dim * 8, 1, k_size=3, stride=1, pad=1, use_bn=False)\n",
        "        self.cls = starGAN_conv_block(conv_dim * 8, num_domains, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "        self.label_cls = starGAN_conv_block(conv_dim * 8, num_classes, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        alpha = 0.01\n",
        "        x = F.leaky_relu(self.conv1(x), alpha)\n",
        "        x = F.leaky_relu(self.conv2(x), alpha)\n",
        "        x = F.leaky_relu(self.conv3(x), alpha)\n",
        "        x = F.leaky_relu(self.conv4(x), alpha)\n",
        "        gan_out = self.gan(x)\n",
        "        cls_out = self.cls(x)\n",
        "        label_cls_out = self.label_cls(x)\n",
        "\n",
        "        return gan_out, cls_out.squeeze(), label_cls_out.squeeze()\n",
        "\n",
        "class starGAN_Generator(nn.Module): # 원본_64\n",
        "    def __init__(self, in_channels=3, num_domains=5, image_size=32, out_channels=3, conv_dim=64):\n",
        "        super(starGAN_Generator, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.embed_layer = nn.Embedding(num_domains, image_size**2)\n",
        "\n",
        "        self.conv1 = starGAN_conv_block(in_channels+1, conv_dim, k_size=5, stride=1, pad=2, use_bn=True) # 64\n",
        "        self.conv2 = starGAN_conv_block(conv_dim, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True) #  64 * 2\n",
        "        self.conv3 = starGAN_conv_block(conv_dim * 2, conv_dim * 4, k_size=4, stride=2, pad=1, use_bn=True) # 64 *4 = 256\n",
        "        self.res4 = starGAN_ResBlock(conv_dim * 4)\n",
        "        self.res5 = starGAN_ResBlock(conv_dim * 4)\n",
        "        self.res6 = starGAN_ResBlock(conv_dim * 4)\n",
        "        self.tconv7 = starGAN_conv_block(conv_dim * 4, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.tconv8 = starGAN_conv_block(conv_dim * 2, conv_dim, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.conv9 = starGAN_conv_block(conv_dim, out_channels, k_size=5, stride=1, pad=2, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, target_dm=None):\n",
        "        if target_dm is None:\n",
        "            target_dm = torch.ones(x.shape[0])\n",
        "        target_dm = target_dm.long()\n",
        "        embed = self.embed_layer(target_dm).reshape([-1, 1, self.image_size, self.image_size])\n",
        "        x = torch.cat((x, embed), dim=1)\n",
        "        #[3, 32, 32] + [1, 32, 32] = [4, 32, 32]\n",
        "        x = F.relu(self.conv1(x)) # [64,32,32]\n",
        "        x = F.relu(self.conv2(x))  # [128,16,16]\n",
        "        x = F.relu(self.conv3(x))# [256, 8, 8]\n",
        "        x = F.relu(self.res4(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.res5(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.res6(x))  # [256, 8, 8]\n",
        "        x = F.relu(self.tconv7(x)) # [128,16,16]\n",
        "        x = F.relu(self.tconv8(x))  # [64,32,32]\n",
        "        x = torch.tanh(self.conv9(x))  # [3,32,32]\n",
        "        return x"
      ],
      "metadata": {
        "id": "CBSjPR9HW--m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MASCN Transceiver"
      ],
      "metadata": {
        "id": "fHVlS4bhXo7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepJSCC-V Model"
      ],
      "metadata": {
        "id": "xPaKQW57XEOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LowerBound(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs, bound):\n",
        "        b = torch.ones_like(inputs) * bound\n",
        "        ctx.save_for_backward(inputs, b)\n",
        "        return torch.max(inputs, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        inputs, b = ctx.saved_tensors\n",
        "        pass_through_1 = inputs >= b\n",
        "        pass_through_2 = grad_output < 0\n",
        "\n",
        "        pass_through = pass_through_1 | pass_through_2\n",
        "        return pass_through.type(grad_output.dtype) * grad_output, None\n",
        "\n",
        "class GDN(nn.Module):\n",
        "    \"\"\"Generalized divisive normalization layer.\n",
        "    y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ch,\n",
        "                 inverse=False,\n",
        "                 beta_min=1e-6,\n",
        "                 gamma_init=0.1,\n",
        "                 reparam_offset=2**-18):\n",
        "        super(GDN, self).__init__()\n",
        "        self.inverse = inverse\n",
        "        self.beta_min = beta_min\n",
        "        self.gamma_init = gamma_init\n",
        "        self.reparam_offset = reparam_offset\n",
        "\n",
        "        self.build(ch)\n",
        "\n",
        "    def build(self, ch):\n",
        "        self.pedestal = self.reparam_offset**2\n",
        "        self.beta_bound = ((self.beta_min + self.reparam_offset**2)**0.5)\n",
        "        self.gamma_bound = self.reparam_offset\n",
        "\n",
        "        # Create beta param\n",
        "        beta = torch.sqrt(torch.ones(ch)+self.pedestal)\n",
        "        self.beta = nn.Parameter(beta)\n",
        "\n",
        "        # Create gamma param\n",
        "        eye = torch.eye(ch)\n",
        "        g = self.gamma_init*eye\n",
        "        g = g + self.pedestal\n",
        "        gamma = torch.sqrt(g)\n",
        "\n",
        "        self.gamma = nn.Parameter(gamma)\n",
        "        self.pedestal = self.pedestal\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        unfold = False\n",
        "        if inputs.dim() == 5:\n",
        "            unfold = True\n",
        "            bs, ch, d, w, h = inputs.size()\n",
        "            inputs = inputs.view(bs, ch, d*w, h)\n",
        "\n",
        "        _, ch, _, _ = inputs.size()\n",
        "\n",
        "        # Beta bound and reparam\n",
        "        beta = LowerBound.apply(self.beta, self.beta_bound)\n",
        "        beta = beta**2 - self.pedestal\n",
        "\n",
        "        # Gamma bound and reparam\n",
        "        gamma = LowerBound.apply(self.gamma, self.gamma_bound)\n",
        "        gamma = gamma**2 - self.pedestal\n",
        "        gamma = gamma.view(ch, ch, 1, 1)\n",
        "\n",
        "        # Norm pool calc\n",
        "        norm_ = nn.functional.conv2d(inputs**2, gamma, beta)\n",
        "        norm_ = torch.sqrt(norm_)\n",
        "\n",
        "        # Apply norm\n",
        "        if self.inverse:\n",
        "            outputs = inputs * norm_\n",
        "        else:\n",
        "            outputs = inputs / norm_\n",
        "\n",
        "        if unfold:\n",
        "            outputs = outputs.view(bs, ch, d, w, h)\n",
        "        return outputs\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "\n",
        "def deconv(in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding = output_padding,bias=False)\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.gdn(out)\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "class deconv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding = 0):\n",
        "        super(deconv_block, self).__init__()\n",
        "        self.deconv = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,  output_padding = output_padding)\n",
        "        self.gdn = nn.GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv(x)\n",
        "        out = self.gdn(out)\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class conv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_conv1x1=False, kernel_size=3, stride=1, padding=1):\n",
        "        super(conv_ResBlock, self).__init__()\n",
        "        self.conv1 = conv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv2 = conv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.use_conv1x1 = use_conv1x1\n",
        "        if use_conv1x1 == True:\n",
        "            self.conv3 = conv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0)\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_conv1x1 == True:\n",
        "            x = self.conv3(x)\n",
        "        out = out+x\n",
        "        out = self.prelu(out)\n",
        "        return out\n",
        "\n",
        "class deconv_ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_deconv1x1=False, kernel_size=3, stride=1, padding=1, output_padding=0):\n",
        "        super(deconv_ResBlock, self).__init__()\n",
        "        self.deconv1 = deconv(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n",
        "        self.deconv2 = deconv(out_channels, out_channels, kernel_size=1, stride = 1, padding=0, output_padding=0)\n",
        "        self.gdn1 = GDN(out_channels)\n",
        "        self.gdn2 = GDN(out_channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.use_deconv1x1 = use_deconv1x1\n",
        "        if use_deconv1x1 == True:\n",
        "            self.deconv3 = deconv(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, output_padding=output_padding)\n",
        "    def forward(self, x, activate_func='prelu'):\n",
        "        out = self.deconv1(x)\n",
        "        out = self.gdn1(out)\n",
        "        out = self.prelu(out)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.gdn2(out)\n",
        "        if self.use_deconv1x1 == True:\n",
        "            x = self.deconv3(x)\n",
        "        out = out+x\n",
        "        if activate_func=='prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif activate_func=='sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Original Existing Works\n",
        "class AF_block(nn.Module):\n",
        "    def __init__(self, Nin, Nh, No):\n",
        "        super(AF_block, self).__init__()\n",
        "        self.fc1 = nn.Linear(Nin+1, Nh)\n",
        "        self.fc2 = nn.Linear(Nh, No)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x, snr):\n",
        "        # out = F.adaptive_avg_pool2d(x, (1,1))\n",
        "        # out = torch.squeeze(out)\n",
        "        # out = torch.cat((out, snr), 1)\n",
        "        if snr.shape[0]>1:\n",
        "            snr = snr.squeeze()\n",
        "        snr = snr.unsqueeze(1)\n",
        "        mu = torch.mean(x, (2, 3))\n",
        "        out = torch.cat((mu, snr), 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.unsqueeze(2)\n",
        "        out = out.unsqueeze(3)\n",
        "        out = out*x\n",
        "        return out\n",
        "\n",
        "# The Encoder model with attention feature blocks\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_conv):\n",
        "        super(Encoder, self).__init__()\n",
        "        enc_N = enc_shape[0]\n",
        "        Nh_AF = Nc_conv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.conv1 = conv_ResBlock(3, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L) # 데이터셋의 채널수에따라서 1이 3으로 바뀌고 3이 1로 바뀔 수 있음\n",
        "        self.conv2 = conv_ResBlock(Nc_conv, Nc_conv, use_conv1x1=True, kernel_size = kernel_sz, stride = 2, padding=padding_L)\n",
        "        self.conv3 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv4 = conv_ResBlock(Nc_conv, Nc_conv, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.conv5 = conv_ResBlock(Nc_conv, enc_N, use_conv1x1=True, kernel_size = kernel_sz, stride = 1, padding=padding_L)\n",
        "        self.AF1 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF2 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF3 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF4 = AF_block(Nc_conv, Nh_AF, Nc_conv)\n",
        "        self.AF5 = AF_block(enc_N, enc_N//2, enc_N)\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = self.conv1(x)\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.conv2(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.conv3(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.conv4(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.conv5(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.flatten(out)\n",
        "        return out\n",
        "\n",
        "# The Decoder model with attention feature blocks\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, enc_shape, kernel_sz, Nc_deconv):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.enc_shape = enc_shape\n",
        "        Nh_AF1 = enc_shape[0]//2\n",
        "        Nh_AF = Nc_deconv//2\n",
        "        padding_L = (kernel_sz-1)//2\n",
        "        self.deconv1 = deconv_ResBlock(self.enc_shape[0], Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv2 = deconv_ResBlock(Nc_deconv, Nc_deconv, use_deconv1x1=True, kernel_size = kernel_sz, stride = 2,  padding=padding_L, output_padding = 1)\n",
        "        self.deconv3 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv4 = deconv_ResBlock(Nc_deconv, Nc_deconv, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "        self.deconv5 = deconv_ResBlock(Nc_deconv, 3, use_deconv1x1=True, kernel_size=kernel_sz, stride=1, padding=padding_L)\n",
        "\n",
        "        self.AF1 = AF_block(self.enc_shape[0], Nh_AF1, self.enc_shape[0])\n",
        "        self.AF2 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF3 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF4 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "        self.AF5 = AF_block(Nc_deconv, Nh_AF, Nc_deconv)\n",
        "    def forward(self, x, snr):\n",
        "        #snr = snr.view(-1, 1)\n",
        "        out = x.view(-1, self.enc_shape[0], self.enc_shape[1], self.enc_shape[2])\n",
        "        out = self.AF1(out, snr)\n",
        "        out = self.deconv1(out)\n",
        "        out = self.AF2(out, snr)\n",
        "        out = self.deconv2(out)\n",
        "        out = self.AF3(out, snr)\n",
        "        out = self.deconv3(out)\n",
        "        out = self.AF4(out, snr)\n",
        "        out = self.deconv4(out)\n",
        "        out = self.AF5(out, snr)\n",
        "        out = self.deconv5(out, 'sigmoid')\n",
        "        return out\n",
        "\n",
        "# Power normalization before transmission\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1)\n",
        "    return np.sqrt(P*z_dim)*z/z_M.t()\n",
        "\n",
        "def Power_norm_complex(z, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    z_com = torch.complex(z[:, 0:z_dim:2], z[:, 1:z_dim:2])\n",
        "    z_com_conj = torch.complex(z[:, 0:z_dim:2], -z[:, 1:z_dim:2])\n",
        "    z_power = torch.sum(z_com*z_com_conj, 1).real\n",
        "    z_M = z_power.repeat(z_dim//2, 1)\n",
        "    z_nlz = np.sqrt(P*z_dim)*z_com/torch.sqrt(z_M.t())\n",
        "    z_out = torch.zeros(batch_size, z_dim).cuda()\n",
        "    z_out[:, 0:z_dim:2] = z_nlz.real\n",
        "    z_out[:, 1:z_dim:2] = z_nlz.imag\n",
        "    return z_out\n",
        "\n",
        "# The (real) AWGN channel\n",
        "def AWGN_channel(x, snr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def AWGN_complex(x, snr, Ps = 1):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    n_I = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    n_R = torch.sqrt(Ps/gamma)*torch.randn(batch_size, length).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "    y = x + noise\n",
        "    return y\n",
        "\n",
        "# Please set the symbol power if it is not a default value\n",
        "def Fading_channel(x, snr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "# Note: if P = 1, the symbol power is 2\n",
        "# If you want to set the average power as 1, please change P as P=1/np.sqrt(2)\n",
        "def Power_norm_VLC(z, cr, P = 1):\n",
        "    batch_size, z_dim = z.shape\n",
        "    Kv = torch.ceil(z_dim*cr).int()\n",
        "    z_power = torch.sqrt(torch.sum(z**2, 1))\n",
        "    z_M = z_power.repeat(z_dim, 1).cuda()\n",
        "    return torch.sqrt(Kv*P)*z/z_M.t()\n",
        "\n",
        "def AWGN_channel_VLC(x, snr, cr, P = 2):\n",
        "    batch_size, length = x.shape\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    mask = mask_gen(length, cr).cuda()\n",
        "    noise = torch.sqrt(P/gamma)*torch.randn(1, length).cuda()\n",
        "    noise = noise*mask\n",
        "    y = x+noise\n",
        "    return y\n",
        "\n",
        "def Fading_channel_VLC(x, snr, cr, P = 2):\n",
        "    gamma = 10 ** (snr / 10.0)\n",
        "    [batch_size, feature_length] = x.shape\n",
        "    K = feature_length//2\n",
        "\n",
        "    mask = mask_gen(K, cr).cuda()\n",
        "    h_I = torch.randn(batch_size, K).cuda()\n",
        "    h_R = torch.randn(batch_size, K).cuda()\n",
        "    h_com = torch.complex(h_I, h_R)\n",
        "    x_com = torch.complex(x[:, 0:feature_length:2], x[:, 1:feature_length:2])\n",
        "    y_com = h_com*x_com\n",
        "\n",
        "    n_I = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    n_R = torch.sqrt(P/gamma)*torch.randn(batch_size, K).cuda()\n",
        "    noise = torch.complex(n_I, n_R)*mask\n",
        "\n",
        "    y_add = y_com + noise\n",
        "    y = y_add/h_com\n",
        "\n",
        "    y_out = torch.zeros(batch_size, feature_length).cuda()\n",
        "    y_out[:, 0:feature_length:2] = y.real\n",
        "    y_out[:, 1:feature_length:2] = y.imag\n",
        "    return y_out\n",
        "\n",
        "def Channel(z, snr, channel_type = 'AWGN'):\n",
        "    z = Power_norm(z)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel(z, snr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel(z, snr)\n",
        "    return z\n",
        "\n",
        "def Channel_VLC(z, snr, cr, channel_type = 'AWGN'):\n",
        "    z = Power_norm_VLC(z, cr)\n",
        "    if channel_type == 'AWGN':\n",
        "        z = AWGN_channel_VLC(z, snr, cr)\n",
        "    elif channel_type == 'Fading':\n",
        "        z = Fading_channel_VLC(z, snr, cr)\n",
        "    return z\n",
        "\n",
        "def mask_gen(N, cr, ch_max = 48):\n",
        "    MASK = torch.zeros(cr.shape[0], N).int()\n",
        "    nc = N//ch_max\n",
        "    for i in range(0, cr.shape[0]):\n",
        "        L_i = nc*torch.round(ch_max*cr[i]).int()\n",
        "        MASK[i, 0:L_i] = 1\n",
        "    return MASK\n",
        "\n",
        "class ADJSCC(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = Channel(z, snr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out\n",
        "\n",
        "# The DeepJSCC_V model, also called ADJSCC_V\n",
        "class ADJSCC_V(nn.Module):\n",
        "    def __init__(self, enc_shape, Kernel_sz, Nc):\n",
        "        super(ADJSCC_V, self).__init__()\n",
        "        self.encoder = Encoder(enc_shape, Kernel_sz, Nc)\n",
        "        self.decoder = Decoder(enc_shape, Kernel_sz, Nc)\n",
        "    def forward(self, x, snr, cr, channel_type = 'AWGN'):\n",
        "        z = self.encoder(x, snr)\n",
        "        z = z*mask_gen(z.shape[1], cr).cuda()\n",
        "        z = Channel_VLC(z, snr, cr, channel_type)\n",
        "        out = self.decoder(z, snr)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9aZV8NyGXGpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MASCN Transceiver"
      ],
      "metadata": {
        "id": "kslXG4mGXkaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module): # MASCN_generative DeepSC\n",
        "    def __init__(self, in_channels=3, num_domains=4, image_size=32, out_channels=3, conv_dim=64, CR=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.embed_layer = nn.Embedding(num_domains, image_size**2)\n",
        "        self.CR=CR\n",
        "        conv_dim_btl = int(conv_dim * 4 *CR)\n",
        "        self.conv_dim=conv_dim\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = conv_block(in_channels+1, conv_dim, k_size=5, stride=1, pad=2, use_bn=True) # 64\n",
        "        self.conv2 = conv_block(conv_dim, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True) #  64*2\n",
        "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4, k_size=4, stride=2, pad=1, use_bn=True) # 64*4 = 256 밑에도 수정일어남\n",
        "        self.res4 = ResBlock(conv_dim * 4)\n",
        "        self.AF4= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res5 = ResBlock(conv_dim * 4)\n",
        "        self.AF5= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.conv5 = conv_block(conv_dim * 4, conv_dim_btl , k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        #self.AF_c_5= AF_block(conv_dim_btl, conv_dim_btl//2, conv_dim_btl)\n",
        "\n",
        "        # Decoder\n",
        "        #self.AF_c_6= AF_block(conv_dim_btl, conv_dim_btl//2, conv_dim_btl)\n",
        "        self.conv6 = conv_block(conv_dim_btl, conv_dim * 4, k_size=3, stride=1, pad=1, use_bn=True)\n",
        "        self.AF6= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res6 = ResBlock(conv_dim * 4)\n",
        "        self.AF7= AF_block(conv_dim * 4, conv_dim*4 //2, conv_dim * 4)\n",
        "        self.res7 = ResBlock(conv_dim * 4)\n",
        "        self.tconv7 = conv_block(conv_dim * 4, conv_dim * 2, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.tconv8 = conv_block(conv_dim * 2, conv_dim, k_size=4, stride=2, pad=1, use_bn=True, transpose=True)\n",
        "        self.conv9 = conv_block(conv_dim, out_channels, k_size=5, stride=1, pad=2, use_bn=False)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, channel_type, snr,  target_dm=None):\n",
        "        if target_dm is None:\n",
        "            target_dm = torch.ones(x.shape[0])\n",
        "        target_dm = target_dm.long()\n",
        "        embed = self.embed_layer(target_dm).reshape([-1, 1, self.image_size, self.image_size])\n",
        "        x = torch.cat((x, embed), dim=1)\n",
        "        x = F.relu(self.conv1(x)) # [12,32,32]\n",
        "        x = F.relu(self.conv2(x))  # [24,16,16]\n",
        "        x = F.relu(self.conv3(x))# [48, 8, 8]\n",
        "        x = F.relu(self.res4(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF4(x, snr))\n",
        "        x = F.relu(self.res5(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF5(x, snr))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        #x = F.relu(self.AF_c_5(x, snr))\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = Channel(x, snr, channel_type)\n",
        "        # 1. binary masking operation (It dosen't work well.. ) has some problem. but, output size of encoder(conv_dim) is lower  -> works well\n",
        "        # 2. AWGN channel -> channel coding layer.\n",
        "        out_size= int(self.conv_dim*4 *self.CR)\n",
        "        x = x.view(-1, out_size, 8, 8) # dependent code\n",
        "        #x = F.relu(self.AF_c_6(x, snr))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.AF6(x, snr))\n",
        "        x = F.relu(self.res6(x))  # [48, 8, 8]\n",
        "        x = F.relu(self.AF7(x, snr))\n",
        "        x = F.relu(self.res7(x))  # [48, 8, 8]\n",
        "\n",
        "        x = F.relu(self.tconv7(x)) # [24,16,16]\n",
        "        x = F.relu(self.tconv8(x))  # [12,32,32]\n",
        "        x = torch.tanh(self.conv9(x))  # [3,32,32]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels=3, num_domains=4, num_classes=10, image_size=32, conv_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = conv_block(channels, conv_dim, use_bn=False)\n",
        "        self.conv2 = conv_block(conv_dim, conv_dim * 2, use_bn=False)\n",
        "        self.conv3 = conv_block(conv_dim * 2, conv_dim * 4, use_bn=False)\n",
        "        self.conv4 = conv_block(conv_dim * 4, conv_dim * 8, use_bn=False)\n",
        "\n",
        "        self.gan = conv_block(conv_dim * 8, 1, k_size=3, stride=1, pad=1, use_bn=False)\n",
        "        self.cls = conv_block(conv_dim * 8, num_domains, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "        self.label_cls = conv_block(conv_dim * 8, num_classes, k_size=image_size//16, stride=1, pad=0, use_bn=False)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        alpha = 0.01\n",
        "        x = F.leaky_relu(self.conv1(x), alpha)\n",
        "        x = F.leaky_relu(self.conv2(x), alpha)\n",
        "        x = F.leaky_relu(self.conv3(x), alpha)\n",
        "        x = F.leaky_relu(self.conv4(x), alpha)\n",
        "        gan_out = self.gan(x)\n",
        "        cls_out = self.cls(x)\n",
        "        label_cls_out = self.label_cls(x)\n",
        "\n",
        "        return gan_out, cls_out.squeeze(), label_cls_out.squeeze()"
      ],
      "metadata": {
        "id": "eaXgV98FXLbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "m1LYTqTgsTZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Settings"
      ],
      "metadata": {
        "id": "6mY1uliFsWFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification'\n",
        "model_path = os.path.join(root, 'model')\n",
        "classifier_path = os.path.join(model_path, 'classifier')\n",
        "\n",
        "DS_NAME = [\"MNIST\", \"MNISTM\", \"SYN\", \"USPS\"]\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "IMGS_TO_DISPLAY = 25\n",
        "\n",
        "CHANNEL = 'AWGN'  # Choose AWGN or Fading\n",
        "N_CHANNELS = 256\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "enc_out_shape = [48, IMAGE_SIZE//4, IMAGE_SIZE//4]"
      ],
      "metadata": {
        "id": "wpuuHmWSsVJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upper Bound (DeepJSSC-V with retrained)\n"
      ],
      "metadata": {
        "id": "QBdja6AcuMca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Upper Bound Setting ---\n",
        "# DeepJSCC-V retrained with full supervision on the target domain (e.g., SYN).\n",
        "# This simulates the optimal case where domain-specific knowledge is fully available.\n",
        "# Used to define the upper bound for performance comparison.\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluation settings\n",
        "CR_test_range = np.arange(0.1, 1, 0.1)\n",
        "SNR_test_range = np.array([3, 10, 18])\n",
        "\n",
        "dataset = []\n",
        "ds_loader = []\n",
        "\n",
        "# Load datasets for each domain\n",
        "for domain_id in range(len(DS_NAME)):\n",
        "    dataset.append(\n",
        "        ImgData(f'/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification/dataset/{DS_NAME[domain_id]}_test.pt',\n",
        "                IMAGE_SIZE, IMAGE_SIZE)\n",
        "    )\n",
        "    ds_loader.append(torch.utils.data.DataLoader(dataset[domain_id],\n",
        "                                                 batch_size=BATCH_SIZE,\n",
        "                                                 shuffle=False))\n",
        "\n",
        "total_iter = 0\n",
        "for SNR_test in SNR_test_range:\n",
        "    for CR_test in CR_test_range:\n",
        "        for domain_id in range(len(DS_NAME)):\n",
        "\n",
        "            current_setting = f'upper bound eval with SNR = {SNR_test} and CR = {CR_test} for {DS_NAME[domain_id]}'\n",
        "            print(f'\\nCurrent eval setting: {current_setting}')\n",
        "            print(\"\\n=================================================================\")\n",
        "\n",
        "            # Load ADJSCC-V model trained specifically on the preperation stage domain\n",
        "            ADJSCCV_path = os.path.join(model_path, 'ADJSCCV', f'{DS_NAME[domain_id]}')\n",
        "            ADJSCCV_name = f'JSCC-V_{DS_NAME[domain_id]}.pt'\n",
        "            ADJSCCV = ADJSCC_V(enc_out_shape, KERNEL_SIZE, N_CHANNELS)\n",
        "            ADJSCCV.load_state_dict(torch.load(os.path.join(ADJSCCV_path, ADJSCCV_name)))\n",
        "            ADJSCCV.to(device).eval()\n",
        "\n",
        "            reconstructed_images = []\n",
        "\n",
        "            for _, (src_img, target_label) in tqdm(enumerate(ds_loader[domain_id]), total=len(ds_loader[domain_id])):\n",
        "                total_iter += 1\n",
        "\n",
        "                src_img = src_img.to(device)\n",
        "\n",
        "                SNR = (SNR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "                CR = (CR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "\n",
        "                # Reconstruct image using semantic encoder-decoder\n",
        "                recon_img = (src_img + 1) / 2  # [-1,1] → [0,1]\n",
        "                recon_img = ADJSCCV(recon_img, SNR, CR, CHANNEL)  # [0,1]\n",
        "                recon_img = (recon_img - 0.5) / 0.5  # 다시 [-1,1]\n",
        "\n",
        "                reconstructed_images.append(recon_img.detach().cpu())\n",
        "\n",
        "            # Optional: Merge all tensors for saving or post-processing\n",
        "            reconstructed_images = torch.cat(reconstructed_images, dim=0)\n",
        "\n",
        "            # Optional: save the first 25 images as a grid (disabled)\n",
        "            # torchvision.utils.save_image(reconstructed_images[:25], f'recon_{DS_NAME[domain_id]}_snr{SNR_test}_cr{CR_test}.png', nrow=5, normalize=True)\n",
        "\n",
        "print('All done! (Classifier removed version)')\n"
      ],
      "metadata": {
        "id": "wsrKApMaubql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lower Bound (DeepJSSC-V w/o retrained)\n"
      ],
      "metadata": {
        "id": "IsvfID4Lsfxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Lower Bound Setting ---\n",
        "# DeepJSCC-V trained only on the source domain (e.g., MNIST).\n",
        "# No domain adaptation is applied to the target domain (e.g., SYN).\n",
        "# This represents the lower bound performance baseline.\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Lower Bound Evaluation ===\n",
        "# DeepJSCC-V trained only on other domains (source domains),\n",
        "# and evaluated on the target domain without domain adaptation.\n",
        "\n",
        "CR_test_range = np.arange(0.1, 1, 0.1)\n",
        "SNR_test_range = np.array([3, 10, 18])\n",
        "\n",
        "# Optional backup log (can be removed if image saving is not needed)\n",
        "with open(os.path.join(root, 'eval_info', 'Lower_bound', 'lower_bound.csv'), 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Domain', 'SNR', 'CR'])\n",
        "\n",
        "dataset = []\n",
        "ds_loader = []\n",
        "\n",
        "# For each target domain, exclude it from the training set and build dataset from other domains\n",
        "for domain_id in range(len(train_path)):\n",
        "    subset = [path for i, path in enumerate(train_path) if i != domain_id]\n",
        "    dataset.append(AllDomainExceptOne(subset, IMAGE_SIZE, IMAGE_SIZE))\n",
        "    ds_loader.append(torch.utils.data.DataLoader(dataset[domain_id],\n",
        "                                                 batch_size=BATCH_SIZE,\n",
        "                                                 shuffle=False))\n",
        "\n",
        "total_iter = 0\n",
        "for SNR_test in SNR_test_range:\n",
        "    for CR_test in CR_test_range:\n",
        "        for domain_id in range(len(DS_NAME)):\n",
        "\n",
        "            current_setting = f'lower bound eval with SNR = {SNR_test} and CR = {CR_test} for {DS_NAME[domain_id]}'\n",
        "            print(f'\\nCurrent eval setting: {current_setting}')\n",
        "            print(\"\\n=================================================================\")\n",
        "\n",
        "            # Load ADJSCC-V model trained specifically on the preperation stage domain\n",
        "            ADJSCCV_path = os.path.join(model_path, 'ADJSCCV', f'{DS_NAME[domain_id]}')\n",
        "            ADJSCCV_name = f'JSCC-V_{DS_NAME[domain_id]}.pt'\n",
        "            ADJSCCV = ADJSCC_V(enc_out_shape, KERNEL_SIZE, N_CHANNELS)\n",
        "            ADJSCCV.load_state_dict(torch.load(os.path.join(ADJSCCV_path, ADJSCCV_name)))\n",
        "            ADJSCCV.to(device).eval()\n",
        "\n",
        "            reconstructed_images = []\n",
        "\n",
        "            for _, (src_img, target_label) in tqdm(enumerate(ds_loader[domain_id]), total=len(ds_loader[domain_id])):\n",
        "                total_iter += 1\n",
        "                src_img = src_img.to(device)\n",
        "\n",
        "                SNR = (SNR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "                CR = (CR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "\n",
        "                # Image reconstruction via the semantic transceiver\n",
        "                recon_img = (src_img + 1) / 2  # [-1,1] → [0,1]\n",
        "                recon_img = ADJSCCV(recon_img, SNR, CR, CHANNEL)  # [0,1]\n",
        "                recon_img = (recon_img - 0.5) / 0.5  # [0,1] → [-1,1]\n",
        "\n",
        "                reconstructed_images.append(recon_img.detach().cpu())\n",
        "\n",
        "            reconstructed_images = torch.cat(reconstructed_images, dim=0)\n",
        "\n",
        "            # Optional: save images\n",
        "            # torchvision.utils.save_image(reconstructed_images[:25], f'recon_LB_{DS_NAME[domain_id]}_snr{SNR_test}_cr{CR_test}.png', nrow=5, normalize=True)\n",
        "\n",
        "            # Log the evaluation configuration (no accuracy logged)\n",
        "            with open(os.path.join(root, 'eval_info', 'Lower_bound', 'lower_bound.csv'), 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([f'{DS_NAME[domain_id]}',\n",
        "                                 SNR_test,\n",
        "                                 CR_test])\n",
        "\n",
        "print('All done! (Lower Bound Classifier Removed)')\n",
        "\n"
      ],
      "metadata": {
        "id": "byPMBDiIucNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MDAN Model"
      ],
      "metadata": {
        "id": "TWmSA5XPzcXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Path Configuration ===\n",
        "starGAN_path = os.path.join(model_path, 'starGAN_w_clf')\n",
        "samples_path = os.path.join(root, 'samples', 'starGAN_w_clf')\n",
        "os.makedirs(samples_path, exist_ok=True)\n",
        "\n",
        "# === Evaluation Settings ===\n",
        "CR_test_range = np.arange(0.1, 1, 0.1)\n",
        "SNR_test_range = np.array([3, 10, 18])\n",
        "\n",
        "total_iter = 0\n",
        "for SNR_test in SNR_test_range:\n",
        "    for CR_test in CR_test_range:\n",
        "        for domain_id in range(len(DS_NAME)):\n",
        "\n",
        "            current_setting = f'starGAN eval with SNR = {SNR_test} and CR = {CR_test} for {DS_NAME[domain_id]}'\n",
        "            print(f'\\nCurrent eval setting: {current_setting}')\n",
        "            print(\"\\n=================================================================\")\n",
        "\n",
        "            # Load StarGAN Generator\n",
        "            starGAN_name = 'gen_100.pkl'\n",
        "            starGAN = starGAN_Generator(num_domains=len(DS_NAME), image_size=IMAGE_SIZE)\n",
        "            starGAN.load_state_dict(torch.load(os.path.join(starGAN_path, starGAN_name)))\n",
        "            starGAN.to(device).eval()\n",
        "\n",
        "            # Load ADJSCC-V for Semantic Communication\n",
        "            ADJSCCV_path = os.path.join(model_path, 'ADJSCCV', f'{DS_NAME[domain_id]}')\n",
        "            ADJSCCV_name = f'JSCC-V_{DS_NAME[domain_id]}.pt'\n",
        "            ADJSCCV = ADJSCC_V(enc_out_shape, KERNEL_SIZE, N_CHANNELS)\n",
        "            ADJSCCV.load_state_dict(torch.load(os.path.join(ADJSCCV_path, ADJSCCV_name)))\n",
        "            ADJSCCV.to(device).eval()\n",
        "\n",
        "            # Load evaluation dataset (without domain labels)\n",
        "            dataset = AllDomainData(IMAGE_SIZE, IMAGE_SIZE, is_training=False, DA_task=False)\n",
        "            ds_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                                    batch_size=BATCH_SIZE,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "            for _, (src_img, _) in tqdm(enumerate(ds_loader), total=len(ds_loader)):\n",
        "                total_iter += 1\n",
        "\n",
        "                src_img = src_img.to(device)\n",
        "                target_domain = torch.full((src_img.shape[0],), domain_id).long().to(device)\n",
        "\n",
        "                # StarGAN domain adaptation\n",
        "                DA_img = starGAN(src_img, target_domain)  # [-1, 1]\n",
        "\n",
        "                # Semantic transmission (ADJSCC-V)\n",
        "                SNR = torch.full((DA_img.shape[0], 1), SNR_test).to(device)\n",
        "                CR = torch.full((DA_img.shape[0], 1), CR_test).to(device)\n",
        "\n",
        "                DA_img_rec = (DA_img + 1) / 2   # [-1,1] → [0,1]\n",
        "                DA_img_rec = ADJSCCV(DA_img_rec, SNR, CR, CHANNEL)  # [0,1]\n",
        "                DA_img_rec = (DA_img_rec - 0.5) / 0.5  # [0,1] → [-1,1]\n",
        "\n",
        "                # Save visualization every 100 steps\n",
        "                if total_iter % 100 == 0:\n",
        "                    generate_imgs(src_img[:IMGS_TO_DISPLAY], len(DS_NAME), starGAN, samples_path, device, total_iter)\n",
        "\n",
        "print('StarGAN evaluation complete (classifier removed).')\n"
      ],
      "metadata": {
        "id": "CLIyFJkcsVMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN based DA Model"
      ],
      "metadata": {
        "id": "3HFzjv2M3qUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Path Configuration ===\n",
        "cycleGAN_path = os.path.join(model_path, 'cycleGAN')\n",
        "\n",
        "# === Evaluation Settings ===\n",
        "CR_test_range = np.arange(0.1, 1, 0.1)\n",
        "SNR_test_range = np.array([3, 10, 18])\n",
        "\n",
        "for SNR_test in SNR_test_range:\n",
        "    for CR_test in CR_test_range:\n",
        "        for dst_domain in range(len(DS_NAME)):\n",
        "\n",
        "            current_setting = f'cycleGAN eval with SNR = {SNR_test} and CR = {CR_test} for {DS_NAME[dst_domain]}'\n",
        "            print(f'\\nCurrent eval setting: {current_setting}')\n",
        "            print(\"\\n=================================================================\")\n",
        "\n",
        "            # Load ADJSCC-V model\n",
        "            ADJSCCV_path = os.path.join(model_path, 'ADJSCCV', f'{DS_NAME[dst_domain]}')\n",
        "            ADJSCCV_name = f'JSCC-V_{DS_NAME[dst_domain]}.pt'\n",
        "            ADJSCCV = ADJSCC_V(enc_out_shape, KERNEL_SIZE, N_CHANNELS)\n",
        "            ADJSCCV.load_state_dict(torch.load(os.path.join(ADJSCCV_path, ADJSCCV_name)))\n",
        "            ADJSCCV.to(device).eval()\n",
        "\n",
        "            # Loop over all source domains except the current target\n",
        "            for src_domain in [i for i in range(len(DS_NAME)) if i != dst_domain]:\n",
        "\n",
        "                print(f'Source domain: {DS_NAME[src_domain]} --> Destination domain: {DS_NAME[dst_domain]}')\n",
        "\n",
        "                # Load CycleGAN Generator\n",
        "                cycleGAN_name = f'gen_{DS_NAME[src_domain]}_{DS_NAME[dst_domain]}.pkl'\n",
        "                cycleGAN = CycleGAN_Generator(in_channels=3, out_channels=3, conv_dim=12)\n",
        "                cycleGAN.load_state_dict(torch.load(os.path.join(cycleGAN_path, cycleGAN_name)))\n",
        "                cycleGAN.to(device).eval()\n",
        "\n",
        "                # Load dataset\n",
        "                src_dataset = ImgData(os.path.join(root, 'dataset', f'{DS_NAME[src_domain]}_test.pt'),\n",
        "                                      IMAGE_SIZE, IMAGE_SIZE)\n",
        "                ds_loader = torch.utils.data.DataLoader(src_dataset,\n",
        "                                                        batch_size=BATCH_SIZE,\n",
        "                                                        shuffle=False)\n",
        "\n",
        "                for _, (src_img, _) in tqdm(enumerate(ds_loader), total=len(ds_loader)):\n",
        "                    src_img = src_img.to(device)\n",
        "\n",
        "                    # Domain adaptation (CycleGAN)\n",
        "                    DA_img = cycleGAN(src_img)\n",
        "\n",
        "                    # Semantic reconstruction (ADJSCC-V)\n",
        "                    SNR = torch.full((DA_img.shape[0], 1), SNR_test).to(device)\n",
        "                    CR = torch.full((DA_img.shape[0], 1), CR_test).to(device)\n",
        "\n",
        "                    DA_img = (DA_img + 1) / 2  # [-1,1] → [0,1]\n",
        "                    DA_img_rec = ADJSCCV(DA_img, SNR, CR, CHANNEL)\n",
        "                    DA_img_rec = (DA_img_rec - 0.5) / 0.5  # [0,1] → [-1,1]\n",
        "\n",
        "                    # At this point, reconstructed images (DA_img_rec) are available\n",
        "                    # You can save or evaluate them here\n",
        "                    # Example: torchvision.utils.save_image(DA_img_rec, ...)\n",
        "\n",
        "print('CycleGAN evaluation complete (classifier removed).')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yu0NIDYu3tYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MASCN Transceiver\n",
        "\n",
        "Since all models are executed sequentially in the Colab environment, some parts of this code conflict with the existing implementation and do not function properly.  \n",
        "This section will be separated and uploaded later as an independent module."
      ],
      "metadata": {
        "id": "yRGmjfA59_qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Test Dataset and Evaluation"
      ],
      "metadata": {
        "id": "UUj40QZebXF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/My Drive/ADJSCC-V_withDA_4digit_classification'\n",
        "model_path = os.path.join(root, 'model')\n",
        "classifier_path = os.path.join(model_path, 'classifier')\n",
        "\n",
        "DS_NAME = [\"MNIST\", \"MNISTM\", \"SYN\", \"USPS\"]\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "IMGS_TO_DISPLAY = 25\n",
        "\n",
        "CHANNEL = 'AWGN'  # Choose AWGN or Fading\n",
        "N_CHANNELS = 256\n",
        "KERNEL_SIZE = 5\n",
        "\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "enc_out_shape = [48, IMAGE_SIZE//4, IMAGE_SIZE//4]\n",
        "\n",
        "# MNSIT-2, MNISTM-2, USPS-2, SYN-2\n",
        "\n",
        "MNIST_ds = ImgData(os.path.join(root, 'dataset', 'MNIST_test.pt'), IMAGE_SIZE, IMAGE_SIZE)\n",
        "MNIST_ds_loader = torch.utils.data.DataLoader(MNIST_ds,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True)\n",
        "\n",
        "MNISTM_ds = ImgData(os.path.join(root, 'dataset', 'MNISTM_test.pt'), IMAGE_SIZE, IMAGE_SIZE)\n",
        "MNISTM_ds_loader = torch.utils.data.DataLoader(MNISTM_ds,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True)\n",
        "\n",
        "USPS_ds = ImgData(os.path.join(root, 'dataset', 'USPS_test.pt'), IMAGE_SIZE, IMAGE_SIZE)\n",
        "USPS_ds_loader = torch.utils.data.DataLoader(USPS_ds,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True)\n",
        "\n",
        "\n",
        "SYN_ds = ImgData(os.path.join(root, 'dataset', 'SYN_test.pt'), IMAGE_SIZE, IMAGE_SIZE)\n",
        "SYN_ds_loader = torch.utils.data.DataLoader(SYN_ds,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        shuffle=True)\n"
      ],
      "metadata": {
        "id": "fJrVs20JbcsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "DS_NAME_dict = {\n",
        "    'MNIST': 0,\n",
        "    'MNISTM': 1,\n",
        "    'SYN': 2,\n",
        "    'USPS': 3\n",
        "}\n",
        "\n",
        "def plot_domain_adaptation_results(src_img, CR_test, SNR_test, src_domain, dst_domains, model_path, device):\n",
        "    if src_img.dim() == 3:\n",
        "        src_img = src_img.unsqueeze(0)\n",
        "\n",
        "    src_img = src_img.to(device)\n",
        "    SNR = (SNR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "    CR = (CR_test * torch.ones(src_img.shape[0], 1)).to(device)\n",
        "\n",
        "    # Initialize figure with an extra column for algorithm names\n",
        "    fig, axs = plt.subplots(3, len(dst_domains) + 2, figsize=(18, 6))  # Added one more column\n",
        "\n",
        "    # Normalize source image for display\n",
        "    src_img_display = (src_img + 1) / 2\n",
        "\n",
        "    # Set labels for the algorithms in the new left-most column\n",
        "    axs[0, 0].text(0.5, 0.5, 'CycleGAN', horizontalalignment='center', verticalalignment='center', fontsize=24, transform=axs[0, 0].transAxes)\n",
        "    axs[0, 0].axis('off')\n",
        "    axs[1, 0].text(0.5, 0.5, 'StarGAN', horizontalalignment='center', verticalalignment='center', fontsize=24, transform=axs[1, 0].transAxes)\n",
        "    axs[1, 0].axis('off')\n",
        "    axs[2, 0].text(0.5, 0.5, 'SCN', horizontalalignment='center', verticalalignment='center', fontsize=24, transform=axs[2, 0].transAxes)\n",
        "    axs[2, 0].axis('off')\n",
        "\n",
        "    # Display source image in both rows\n",
        "    axs[0, 1].imshow(to_pil_image(src_img_display[0].cpu().squeeze()))\n",
        "    axs[0, 1].set_title(f'Input ({src_domain}, SNR = {SNR_test}, CR = {CR_test})')\n",
        "    axs[0, 1].axis('off')\n",
        "\n",
        "    axs[1, 1].imshow(to_pil_image(src_img_display[0].cpu().squeeze()))\n",
        "    axs[1, 1].set_title(f'Input ({src_domain}, SNR = {SNR_test}, CR = {CR_test})')\n",
        "    axs[1, 1].axis('off')\n",
        "\n",
        "    axs[2, 1].imshow(to_pil_image(src_img_display[0].cpu().squeeze()))\n",
        "    axs[2, 1].set_title(f'Input ({src_domain}, SNR = {SNR_test}, CR = {CR_test})')\n",
        "    axs[2, 1].axis('off')\n",
        "\n",
        "    # Iterate over all destination domains\n",
        "    for i, dst_domain in enumerate(dst_domains):\n",
        "\n",
        "        # Load CycleGAN\n",
        "        cycleGAN_path = os.path.join(model_path, 'cycleGAN')\n",
        "        cycleGAN_name = f'gen_{src_domain}_{dst_domain}.pkl'\n",
        "        cycleGAN = CycleGAN_Generator(in_channels=3, out_channels=3, conv_dim=12)\n",
        "        cycleGAN.load_state_dict(torch.load(os.path.join(cycleGAN_path, cycleGAN_name)))\n",
        "        cycleGAN.to(device).eval()\n",
        "\n",
        "        # Load StarGAN\n",
        "        starGAN_path = os.path.join(model_path, 'starGAN_w_clf')\n",
        "        starGAN_name = 'gen_100.pkl'\n",
        "        starGAN = starGAN_Generator(num_domains=len(DS_NAME_dict), image_size=IMAGE_SIZE)\n",
        "        starGAN.load_state_dict(torch.load(os.path.join(starGAN_path, starGAN_name)))\n",
        "        starGAN.to(device).eval()\n",
        "\n",
        "        # Load SCN\n",
        "        SCN_model_path = os.path.join('/content/drive/My Drive/StarGAN/', f'model_256_AWGN_CR_{CR_test:.1f}_AFB')\n",
        "        SCN = Generator(in_channels=3, num_domains=len(DS_NAME_dict), image_size=IMAGE_SIZE, out_channels=3, conv_dim=64, CR=CR_test)\n",
        "        SCN.load_state_dict(torch.load(os.path.join(SCN_model_path, f'gen_AFB_100_{CR_test:.1f}.pkl')))\n",
        "        SCN.to(device).eval()\n",
        "\n",
        "        target_domain = (torch.zeros(src_img.shape[0]) + DS_NAME_dict[dst_domain]).long().to(device)\n",
        "\n",
        "        # Load ADJSCCV\n",
        "        ADJSCCV_path = os.path.join(model_path, 'ADJSCCV', dst_domain)\n",
        "        ADJSCCV_name = f'JSCC-V_{dst_domain}.pt'\n",
        "        ADJSCCV = ADJSCC_V(enc_out_shape, KERNEL_SIZE, N_CHANNELS)\n",
        "        ADJSCCV.load_state_dict(torch.load(os.path.join(ADJSCCV_path, ADJSCCV_name)))\n",
        "        ADJSCCV.to(device).eval()\n",
        "\n",
        "        # Domain adaptation using CycleGAN\n",
        "        DA_img_cycleGAN = cycleGAN(src_img)\n",
        "        DA_img_rec_cycleGAN = (DA_img_cycleGAN + 1) / 2\n",
        "        DA_img_rec_cycleGAN = ADJSCCV(DA_img_rec_cycleGAN, SNR, CR, 'AWGN')\n",
        "        axs[0, i + 2].imshow(to_pil_image(DA_img_rec_cycleGAN[0].cpu().squeeze()))\n",
        "        axs[0, i + 2].set_title(f'{src_domain} to {dst_domain}')\n",
        "        axs[0, i + 2].axis('off')\n",
        "\n",
        "        # Domain adaptation using StarGAN\n",
        "        DA_img_starGAN = starGAN(src_img, target_domain)\n",
        "        DA_img_rec_starGAN = (DA_img_starGAN + 1) / 2\n",
        "        DA_img_rec_starGAN = ADJSCCV(DA_img_rec_starGAN, SNR, CR, 'AWGN')\n",
        "        axs[1, i + 2].imshow(to_pil_image(DA_img_rec_starGAN[0].cpu().squeeze()))\n",
        "        axs[1, i + 2].set_title(f'{src_domain} to {dst_domain}')\n",
        "        axs[1, i + 2].axis('off')\n",
        "\n",
        "        # Domain adaptation using SCN\n",
        "        DA_img_rec_SCN = SCN(src_img, 'AWGN', SNR, target_domain) # [-1, 1]\n",
        "        DA_img_rec_SCN = (DA_img_rec_SCN + 1) / 2 # [0, 1]\n",
        "\n",
        "        axs[2, i + 2].imshow(to_pil_image(DA_img_rec_SCN[0].cpu().squeeze()))\n",
        "        axs[2, i + 2].set_title(f'{src_domain} to {dst_domain}')\n",
        "        axs[2, i + 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RWFO69pubhJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SNR_test = 3\n",
        "CR_test = 0.2\n",
        "\n",
        "for _, (src_img, _) in tqdm(enumerate(MNIST_ds_loader), total=len(MNIST_ds_loader)):\n",
        "    break\n",
        "\n",
        "for _, (src_img2, _) in tqdm(enumerate(MNISTM_ds_loader), total=len(MNISTM_ds_loader)):\n",
        "    break\n",
        "\n",
        "for _, (src_img3, _) in tqdm(enumerate(SYN_ds_loader), total=len(SYN_ds_loader)):\n",
        "    break\n",
        "\n",
        "plot_domain_adaptation_results(src_img=src_img,\n",
        "                               CR_test=CR_test,\n",
        "                               SNR_test=SNR_test,\n",
        "                               src_domain='MNIST',\n",
        "                               dst_domains=['MNISTM', 'SYN', 'USPS'],\n",
        "                               model_path=model_path,\n",
        "                               device=device)\n",
        "\n",
        "plot_domain_adaptation_results(src_img=src_img2,\n",
        "                               CR_test=CR_test,\n",
        "                               SNR_test=SNR_test,\n",
        "                               src_domain='MNISTM',\n",
        "                               dst_domains=['MNIST', 'SYN', 'USPS'],\n",
        "                               model_path=model_path,\n",
        "                               device=device)\n",
        "\n",
        "plot_domain_adaptation_results(src_img=src_img3,\n",
        "                               CR_test=CR_test,\n",
        "                               SNR_test=SNR_test,\n",
        "                               src_domain='SYN',\n",
        "                               dst_domains=['MNIST', 'MNISTM', 'USPS'],\n",
        "                               model_path=model_path,\n",
        "                               device=device)"
      ],
      "metadata": {
        "id": "EFBO1Pyybiyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def save_images(images, labels, dataset_name, root_path):\n",
        "    # Create directory if it does not exist\n",
        "    save_path = os.path.join(root_path, 'qualitative_comparison_test_data', dataset_name)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    for idx, (img, lbl) in enumerate(zip(images, labels)):\n",
        "        # Ensure the image tensor is in the correct shape (C, H, W)\n",
        "        if img.dim() == 3 and img.shape[0] != 3:\n",
        "            img = img.squeeze(0)  # For grayscale, remove the channel if it's 1xHxW\n",
        "        elif img.dim() == 3 and img.shape[0] == 3:\n",
        "            img = img.permute(1, 2, 0)  # For RGB, change from CxHxW to HxWxC\n",
        "\n",
        "        # Convert back to CxHxW for saving\n",
        "        if img.dim() == 3 and img.shape[2] == 3:\n",
        "            img = img.permute(2, 0, 1)  # Only if previously permuted to HxWxC\n",
        "\n",
        "        # Convert tensor to PIL Image\n",
        "        img_pil = TF.to_pil_image(img)\n",
        "        img_path = os.path.join(save_path, f'image_{idx}_label_{lbl}.png')\n",
        "        img_pil.save(img_path)  # Save the image\n",
        "        print(f'Saved {img_path}')\n",
        "\n",
        "\n",
        "# Define the root directory\n",
        "\n",
        "# Dataset names for identification\n",
        "dataset_names = ['MNIST', 'MNISTM', 'USPS', 'SYN']\n",
        "\n",
        "# Save two samples from each dataset\n",
        "for (images, labels), name in zip(zip(sample_images, sample_labels), dataset_names):\n",
        "    save_images(images, labels, name, root)\n"
      ],
      "metadata": {
        "id": "ptcVbm1Ib3a5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}